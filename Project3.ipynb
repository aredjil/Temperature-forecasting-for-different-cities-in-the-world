{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature forecasting for different cities in the world\n",
    "\n",
    "## Introduction\n",
    "For this project you are asked to analyze three datasets, called respectively:\n",
    "1. pollution_us_2000_2016.csv\n",
    "2. greenhouse_gas_inventory_data_data.csv\n",
    "3. GlobalLandTemperaturesByCity.csv\n",
    "\n",
    "You are asked to extract from dataset 2 only the US countries (for which we have info in the other datasets) and to perform the following tasks:\n",
    "- to measure how pollution and temperature create cluster tracing the high populated cities in the world\n",
    "- to analyze the correlation between pollution data and temperature change.\n",
    "- to predict the yearly temperature change of a given city over a given time period, using the <b>ARIMA model</b> for <b>time series forecasting</b>, that is a model for time series forecasting integrating AR models with Moving Average.\n",
    "- (OPTIONAL) rank the 5 cities that will have a highest temperature change in US\n",
    "\n",
    "\n",
    "### TASK1 :Cluster Analysis\n",
    "You use K-means or DBSCAN to perform the cluster analysis, and create a new dataset where the cities are associated to the different identified clusters\n",
    "\n",
    "### TASK 2: Correlation Analysis\n",
    "\n",
    "You measure the correlation between:\n",
    "- temperature and latitude\n",
    "- temperature and pollution\n",
    "- temperature change (difference between the average temperature measured over the last 3 years and the previous temperature) and pollution\n",
    "\n",
    "\n",
    "### TASK 3: Predicting the Temperature of a Given City across a Specified Time Period\n",
    "After reading the data in the temperature data set, for each city cluster, before applying the ARIMA model you perform the following steps:\n",
    "\n",
    "- EDA\n",
    "- data cleaning and preprocessing (Converting the 'dt' (date) column to DateTime format, removing NaN)\n",
    "- feature selection\n",
    "- make the time-series stationary\n",
    "- check for stationarity : Calculating the Augmented Dickey-Fuller Test statistic \n",
    "- identify the (p, q) order of the ARIMA model using ACF partial autocorrelation plot\n",
    "\n",
    "Then:\n",
    "\n",
    "-fit the ARIMA model using the calculated p, q values.\n",
    "-calculate the MSE with respect to the true temp. measurements to estimate the performance of the model\n",
    "\n",
    "\n",
    "NOTE: ARIMA models need the data to be stationary i.e. the data must not exhibit trend and/or seasonality. To identify and remove trend and seasonality, we can use\n",
    "- seasonal decomposition\n",
    "- differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMAResults\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "from nfunctions import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the csv file containing the polluters \n",
    "# df_pollution = load_data(\"Project_3/data/data-project3/pollution_us_2000_2016.csv\")\n",
    "# df_temp = load_data(\"Project_3/data/data-project3/GlobalLandTemperaturesByCity.csv\")\n",
    "# df_ghg = load_data(\"Project_3/data/data-project3/greenhouse_gas_inventory_data_data(1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       City   Latitude   Longitude  Pollution  CO2        Date\n",
      "0    Hawaii  43.277245 -102.871518        199  321  2024-12-12\n",
      "1  Delaware  39.551978  -97.648389        275  314  2024-09-03\n",
      "2   Florida  45.314368  -94.745129        411  326  2024-03-04\n",
      "3   Florida  29.682770 -112.736343         88  477  2024-11-30\n",
      "4   Alabama  27.350475  -88.924688        161  456  2023-04-19\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# List of U.S. states\n",
    "states = [\n",
    "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\",\n",
    "    \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\",\n",
    "    \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n",
    "    \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n",
    "    \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
    "]\n",
    "\n",
    "# Function to generate random dates within a range\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    return start + timedelta(days=random_days)\n",
    "\n",
    "# Define date range\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2025, 1, 1)\n",
    "\n",
    "# Generate data\n",
    "num_samples = 1000\n",
    "data = {\n",
    "    \"City\": [random.choice(states) for _ in range(num_samples)],\n",
    "    \"Latitude\": [round(random.uniform(24.396308, 49.384358), 6) for _ in range(num_samples)],  # USA Latitude range\n",
    "    \"Longitude\": [round(random.uniform(-125.000000, -66.934570), 6) for _ in range(num_samples)],  # USA Longitude range\n",
    "    \"Pollution\": [random.randint(10, 500) for _ in range(num_samples)],  # Random pollution level\n",
    "    \"CO2\": [random.randint(300, 500) for _ in range(num_samples)],  # CO2 levels in ppm\n",
    "    \"Date\": [random_date(start_date, end_date).strftime('%Y-%m-%d') for _ in range(num_samples)],\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SECTION 3: ARIMA model for temperature forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although we can determine p, q values manually by looking at the ACF and PACF plots for a given city, we must automate the process\n",
    "#(OPTIONAL) To automate the process, we must perform a grid search over different values of p and q and choose the ARIMA model for which the AIC and BIC values are minimum\n",
    "\n",
    "p_range = q_range = list(range(0,#))  # taking values from 0 to # (decide this looking at PACF)\n",
    "\n",
    "aic_values = []\n",
    "bic_values = []\n",
    "pq_values = []\n",
    "\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        try:\n",
    "            model = ARIMA(city_df, order=(p, d, q))\n",
    "            results = model.fit(disp=-1)\n",
    "            aic_values.append(ARMAResults.aic(results))\n",
    "            bic_values.append(ARMAResults.bic(results))\n",
    "            pq_values.append((p, q))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "best_pq = pq_values[aic_values.index(min(aic_values))]  # (p,q) corresponding to lowest AIC score\n",
    "print(\"(p,q) corresponding to lowest AIC score: \", best_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting an ARIMA model with chosen p, d, q values and calculating the mean squared error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "arima_model = ARIMA(city_df, order=(best_pq[0], 0, best_pq[1])).fit()\n",
    "predictions = arima_model.predict(start=0, end=len(city_df)-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "write here the report for the project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
