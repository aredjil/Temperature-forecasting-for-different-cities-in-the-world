{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature forecasting for different cities in the world\n",
    "\n",
    "## Introduction\n",
    "For this project you are asked to analyze three datasets, called respectively:\n",
    "1. pollution_us_2000_2016.csv\n",
    "2. greenhouse_gas_inventory_data_data.csv\n",
    "3. GlobalLandTemperaturesByCity.csv\n",
    "\n",
    "You are asked to extract from dataset 2 only the US countries (for which we have info in the other datasets) and to perform the following tasks:\n",
    "- to measure how pollution and temperature create cluster tracing the high populated cities in the world\n",
    "- to analyze the correlation between pollution data and temperature change.\n",
    "- to predict the yearly temperature change of a given city over a given time period, using the <b>ARIMA model</b> for <b>time series forecasting</b>, that is a model for time series forecasting integrating AR models with Moving Average.\n",
    "- (OPTIONAL) rank the 5 cities that will have a highest temperature change in US\n",
    "\n",
    "\n",
    "### TASK1 :Cluster Analysis\n",
    "You use K-means or DBSCAN to perform the cluster analysis, and create a new dataset where the cities are associated to the different identified clusters\n",
    "\n",
    "### TASK 2: Correlation Analysis\n",
    "\n",
    "You measure the correlation between:\n",
    "- temperature and latitude\n",
    "- temperature and pollution\n",
    "- temperature change (difference between the average temperature measured over the last 3 years and the previous temperature) and pollution\n",
    "\n",
    "\n",
    "### TASK 3: Predicting the Temperature of a Given City across a Specified Time Period\n",
    "After reading the data in the temperature data set, for each city cluster, before applying the ARIMA model you perform the following steps:\n",
    "\n",
    "- EDA\n",
    "- data cleaning and preprocessing (Converting the 'dt' (date) column to DateTime format, removing NaN)\n",
    "- feature selection\n",
    "- make the time-series stationary\n",
    "- check for stationarity : Calculating the Augmented Dickey-Fuller Test statistic \n",
    "- identify the (p, q) order of the ARIMA model using ACF partial autocorrelation plot\n",
    "\n",
    "Then:\n",
    "\n",
    "-fit the ARIMA model using the calculated p, q values.\n",
    "-calculate the MSE with respect to the true temp. measurements to estimate the performance of the model\n",
    "\n",
    "\n",
    "NOTE: ARIMA models need the data to be stationary i.e. the data must not exhibit trend and/or seasonality. To identify and remove trend and seasonality, we can use\n",
    "- seasonal decomposition\n",
    "- differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkmeans_clustering.py\\n\\nThis script performs K-means clustering on the dataset \"averaged_output.csv\" which contains environmental data.\\nIt supports two modes of data preparation:\\n  - \"city-year\": Each row is a unique city-year pair.\\n  - \"city\": Aggregated data per city (averaging numerical features over all years).\\n\\nSelected features for clustering are:\\n  - AverageTemperature\\n  - NO2 Mean\\n  - SO2 Mean\\n  - CO Mean\\n  - Avg_CO2_natural_pross\\n\\nOptionally, you can include the geographic features (Latitude and Longitude) by setting a flag.\\n\\nThe script scales the features using StandardScaler, evaluates the clustering performance using the Elbow method\\nand Silhouette Score (with plots saved as PNG files), performs K-means clustering, visualizes clusters with PCA,\\nand saves the clustering results to a CSV file.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "kmeans_clustering.py\n",
    "\n",
    "This script performs K-means clustering on the dataset \"averaged_output.csv\" which contains environmental data.\n",
    "It supports two modes of data preparation:\n",
    "  - \"city-year\": Each row is a unique city-year pair.\n",
    "  - \"city\": Aggregated data per city (averaging numerical features over all years).\n",
    "\n",
    "Selected features for clustering are:\n",
    "  - AverageTemperature\n",
    "  - NO2 Mean\n",
    "  - SO2 Mean\n",
    "  - CO Mean\n",
    "  - Avg_CO2_natural_pross\n",
    "\n",
    "Optionally, you can include the geographic features (Latitude and Longitude) by setting a flag.\n",
    "\n",
    "The script scales the features using StandardScaler, evaluates the clustering performance using the Elbow method\n",
    "and Silhouette Score (with plots saved as PNG files), performs K-means clustering, visualizes clusters with PCA,\n",
    "and saves the clustering results to a CSV file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating optimal number of clusters...\n",
      "Elbow Method and Silhouette Score plots have been saved as 'elbow_method.png' and 'silhouette_scores.png'.\n",
      "Performing K-means clustering with 3 clusters...\n",
      "Cluster visualization saved as 'clusters.png'.\n",
      "Clustering results exported to city_year_clusters.csv\n"
     ]
    }
   ],
   "source": [
    "df_prepared, labels = run_clustering(mode=\"city-year\", include_location=True, n_clusters=3, max_k=10, input_file=\"averaged_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic clusters visualization saved as 'geographic_clusters.png'.\n"
     ]
    }
   ],
   "source": [
    "visualize_geographic_clusters(df_prepared, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SECTION 3: ARIMA model for temperature forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although we can determine p, q values manually by looking at the ACF and PACF plots for a given city, we must automate the process\n",
    "#(OPTIONAL) To automate the process, we must perform a grid search over different values of p and q and choose the ARIMA model for which the AIC and BIC values are minimum\n",
    "\n",
    "p_range = q_range = list(range(0,#))  # taking values from 0 to # (decide this looking at PACF)\n",
    "\n",
    "aic_values = []\n",
    "bic_values = []\n",
    "pq_values = []\n",
    "\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        try:\n",
    "            model = ARIMA(city_df, order=(p, d, q))\n",
    "            results = model.fit(disp=-1)\n",
    "            aic_values.append(ARMAResults.aic(results))\n",
    "            bic_values.append(ARMAResults.bic(results))\n",
    "            pq_values.append((p, q))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "best_pq = pq_values[aic_values.index(min(aic_values))]  # (p,q) corresponding to lowest AIC score\n",
    "print(\"(p,q) corresponding to lowest AIC score: \", best_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting an ARIMA model with chosen p, d, q values and calculating the mean squared error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "arima_model = ARIMA(city_df, order=(best_pq[0], 0, best_pq[1])).fit()\n",
    "predictions = arima_model.predict(start=0, end=len(city_df)-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "write here the report for the project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
