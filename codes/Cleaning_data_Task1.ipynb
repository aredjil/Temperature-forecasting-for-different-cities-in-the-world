{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMAResults\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "from gustavo_functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets (make sure to update the file paths if necessary)\n",
    "global_df = pd.read_csv(\"data-project3/GlobalLandTemperaturesByCity.csv\")\n",
    "#green_df = pd.read_csv(\"data-project3/greenhouse_gas_inventory_data_data(1).csv\")\n",
    "#green_df = pd.read_csv(\"new_green.csv\")\n",
    "green_df = green\n",
    "pollution_df = pd.read_csv(\"data-project3/pollution_us_2000_2016.csv\")\n",
    "\n",
    "\n",
    "### 1 Cleaning the GlobalLandTemperaturesByCity dataset\n",
    "# Filter only data for the United States\n",
    "global_df = global_df[global_df[\"Country\"] == \"United States\"].copy()\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "global_df[\"dt\"] = pd.to_datetime(global_df[\"dt\"])\n",
    "\n",
    "# Extract the year from the date column\n",
    "global_df[\"Year\"] = global_df[\"dt\"].dt.year\n",
    "\n",
    "# Remove NaN values in the temperature column\n",
    "global_df.dropna(subset=[\"AverageTemperature\"], inplace=True)\n",
    "\n",
    "# Group by city and year, calculating the annual average temperature\n",
    "global_clean = global_df.groupby([\"City\", \"Year\"])['AverageTemperature'].mean().reset_index()\n",
    "\n",
    "# Extract unique latitude values per city\n",
    "latitude_df = global_df[[\"City\", \"Latitude\",\"Longitude\"]].drop_duplicates()\n",
    "\n",
    "### 2 Cleaning the Greenhouse Gas dataset\n",
    "# Filter only \"United States of America\"\n",
    "#green_df = green_df[green_df[\"country_or_area\"] == \"United States of America\"].copy()\n",
    "\n",
    "# Remove NaN values in the value column\n",
    "green_df.dropna(subset=[\"value\"], inplace=True)\n",
    "\n",
    "# Keep only key columns\n",
    "green_clean = green_df[[\"year\", \"value\"]] #green_df[[\"year\", \"value\", \"category\"]]\n",
    "\n",
    "### 3 Cleaning the Pollution dataset\n",
    "# Convert the date column to datetime format\n",
    "pollution_df[\"Date Local\"] = pd.to_datetime(pollution_df[\"Date Local\"])\n",
    "\n",
    "# Extract the year from the date column\n",
    "pollution_df[\"Year\"] = pollution_df[\"Date Local\"].dt.year\n",
    "\n",
    "# Select key columns\n",
    "pollution_clean = pollution_df[[\"City\", \"Year\", \"NO2 Mean\", \"SO2 Mean\", \"CO Mean\"]]\n",
    "\n",
    "# Average pollution values by city and year\n",
    "pollution_clean = pollution_clean.groupby([\"City\", \"Year\"]).mean().reset_index()\n",
    "\n",
    "###  Merging the datasets\n",
    "# Merge temperature and pollution data\n",
    "merged_df = pd.merge(global_clean, pollution_clean, on=[\"City\", \"Year\"], how=\"inner\")\n",
    "\n",
    "# Merge with greenhouse gas data\n",
    "final_df = pd.merge(merged_df, green_clean, left_on=\"Year\", right_on=\"year\", how=\"inner\")\n",
    "\n",
    "# Drop the duplicate year column\n",
    "final_df.drop(columns=[\"year\"], inplace=True)\n",
    "\n",
    "# Merge latitude data\n",
    "final_df = pd.merge(final_df, latitude_df, on=\"City\", how=\"left\")\n",
    "\n",
    "# Convert latitude values to numeric format\n",
    "final_df[\"Latitude\"] = final_df[\"Latitude\"].str.replace(\"N\", \"\").str.replace(\"S\", \"-\").astype(float)\n",
    "\n",
    "# Convert longitude to numeric format\n",
    "final_df[\"Longitude\"] = final_df[\"Longitude\"].apply(lambda x: float(x[:-1]) * (-1 if x[-1] == 'W' else 1))\n",
    "\n",
    "###\n",
    "final_df = final_df.rename(columns={'value': 'CO2-natural-pross'})\n",
    "\n",
    "# Save the cleaned and merged dataset in CSV format\n",
    "final_df.to_csv(\"cleaned_temperature_pollution_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load your green dataset\n",
    "green = pd.read_csv(\"data-project3/greenhouse_gas_inventory_data_data(1).csv\")\n",
    "\n",
    "# Drop the 'country_or_area' column\n",
    "green = green.drop(columns=[\"country_or_area\"])\n",
    "\n",
    "# Load the dataset containing cities \n",
    "cities_df = pd.read_csv(\"data-project3/pollution_us_2000_2016.csv\")\n",
    "\n",
    "# Extract the unique cities from the 'City' column\n",
    "unique_cities = cities_df[\"City\"].unique()\n",
    "\n",
    "# Randomly assign a city from the unique cities to each row in the green dataset.\n",
    "green[\"cities\"] = np.random.choice(unique_cities, size=len(green))\n",
    "\n",
    "# Save the modified dataset to a new CSV file called 'new_green.csv'\n",
    "green.to_csv(\"new_green.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
